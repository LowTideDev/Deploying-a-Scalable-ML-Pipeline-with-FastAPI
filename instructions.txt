Step 1: GitHub Action
Lesson
In the first step, set up GitHub Actions on your repository. You can use one of the pre-made GitHub Actions. It at least runs pytest and flake8 on push and requires both to pass without error.

Make sure you set up the GitHub Action to have the same version of Python as you used in development.
PyTest must pass. By the time the project is done, there should be at least three tests that must pass without errors.
Flake8 must pass without errors.
Flake8 is a popular tool for Python code to provide a comprehensive set of checks for code quality and style. It's considered a best practice to include flake8 in the GitHub Actions for Python.
Most pre-made GitHub action templates for Python include flake8 by default.
At the end of the project, take a screenshot of the GitHub Action showing the passing workflows. Name it continuous_integration.png and upload it to the screenshots/ folder.

Step 2: Data
Lesson
Before training the model, get familiar with the data first. In this step, you will look at the data used to train the model and understand the preprocessing scripts provided.

Inspect the data
Download census.csv from the data folder in the starter repository. Information on the dataset can be found here(opens in a new tab).

Get familiar with the data with your favorite tools.

Preprocess the data
Check the ml/data.py file to understand the data preprocess function. You will use this function to process the data before training.

Step 3: Model
Lesson
Now it's time to build useful functions to train and test the model. In this step, you will use the starter code provided to implement a few functions. These functions will be used to build the ML pipeline later.

Implement required functions
Using the starter code in ml/model.py, complete any functions that have been started to train, make an inference, and save a model. Note the TODO comments # TODO. That's where you will add the code.

The functions you need to complete are:

train_model: train and return a model
inference: run model inferences and return the predictions
save_model: save a model or any categorical encoders
load_model: load a model or any categorical encoders
performance_on_categorical_slice: computes the metrics on a slice of the data
Hint: performance_on_categorical_slice is a function that computes the performance metrics when the value of a given feature is held fixed. E.g., for education, it would print out the model metrics for data with a particular value for education.

Don't forget to add the necessary imports.

Step 4: ML Pipeline
Lesson
You've written all the necessary functions, it's time to use these functions to construct a machine learning pipeline. In this step, you will fill in all the missing pieces in the pipeline.

Implement ML pipeline
Use the starter code train_model.py to write a script that takes the data, processes it, trains the model, and saves the model and the encoder. This script must use the functions you have written in the ml/model.py and ml/data.py. Note the TODO comments # TODO. That's where you will add the code. For example:

# TODO: split the provided data to have a train dataset and a test dataset
Here are the steps that you will implement in the pipeline:

Load the census.csv data.
The data should be split into a training dataset and a test dataset.
Use the process_data function provided to process both the test and the training data.
Use the train_model function to train the model on the training dataset.
Use the inference function to run the model inferences on the test dataset.
Computes performance on data slices using the performance_on_categorical_slice function. And save the output to slice_output.txt.
Hint: when computing performance on data slices, you should have one set of outputs for every single unique value for every categorical feature. You will use for loops to iterate through the categorical features and iterate through the distinct values of that feature. Then print out the model metrics for each value.

Run the ML pipeline train_model.py
> python train_model.py
It will output log messages and performance metrics in the terminal, such as:

Model saved to model/model.pkl
Model saved to model/encoder.pkl
Loading model from model/model.pkl
Precision: 0.7376 | Recall: 0.6288 | F1: 0.6789
It will also store the performance of model slices on a slice_output.txt. The output in the text file will look similar to this:

Precision: 0.5000 | Recall: 0.4762 | F1: 0.4878
workclass: Federal-gov, Count: 188
Precision: 0.8197 | Recall: 0.7353 | F1: 0.7752
workclass: Local-gov, Count: 430
Precision: 0.6792 | Recall: 0.5538 | F1: 0.6102
workclass: Never-worked, Count: 1
Precision: 1.0000 | Recall: 1.0000 | F1: 1.0000
workclass: Private, Count: 4,595
Precision: 0.7381 | Recall: 0.6245 | F1: 0.6766
workclass: Self-emp-inc, Count: 201
Precision: 0.7500 | Recall: 0.8182 | F1: 0.7826
workclass: Self-emp-not-inc, Count: 495
Precision: 0.7333 | Recall: 0.5203 | F1: 0.6087
workclass: State-gov, Count: 248
Precision: 0.8136 | Recall: 0.7059 | F1: 0.7559
Document your model
Document your model using the provided template model_card.md. You should address every section of the template.

The model card should be written in complete sentences and include metrics on model performance. Please include both the metrics used and your model's performance on those metrics.

Step 4: Unit Test
Lesson
It's a good practice to put some tests that verify that the data and the model work as intended. In this step, you will write a few tests to test the ML functions and the data.

Unit tests
In the test_ml.py, write at least 3 unit tests. Unit testing ML can be challenging due to the stochasticity. Here are some unit test ideas for you to implement. But you are encouraged to come up with your unit tests and implement them instead.

If any ML functions return the expected type of result.
If the ML model uses the expected algorithm.
If the computing metrics functions return the expected value.
If the training and test datasets have the expected size or data type.
Run the unit test test_ml.py:
> pytest test_ml.py -v
If all the test passes, you will see messages similar to:

test_ml.py::test_apply_labels PASSED               [ 33%]
test_ml.py::test_train_model PASSED                [ 66%]
test_ml.py::test_compute_model_metrics PASSED      [100%]
Take a screenshot of your terminal showing all tests are passed. Name it unit_test.png and upload it to the screenshots/ folder.

Step 4: API
Lesson
With a working machine learning pipeline, now it's time to deploy it. In this step, you will create a RESTful API and interact with it.

Create a RESTful API
In the main.py, create a RESTful API using FastAPI. Note the TODO comments in the script. For example:

# TODO: create a GET on the root giving a welcome message
The following must be implemented:

Create a RESTful API using FastAPI
Create a GET request on the root giving a welcome message.
Create a POST request on a different path that makes model inference.
Create GET and POST requests
In the local_api.py, write a script that uses the requests module to do one GET request and one POST request on the API. Note the TODO comments in the script. For example:

# TODO: send a GET using the URL
The following must be implemented:

Send a GET and print the status code and the welcome message
Send a POST and print the status code and the result
Interact with the API locally
In the terminal, run the app using the following:

> uvicorn main:app --reload
Once the API is running, open another terminal, and run the local_api.py to interact with the API. If your API and the requests are implemented correctly, you should see something similar to:

Status Code: 200
Result: Hello from the API!
Status Code: 200
Result: <=50K
Note: the result from POST should be either > 50K or <= 50K.

Take a screenshot of the terminal output, including the GET and the POST responses. Name it local_api.png and add it to the screenshots/ folder.

Step 5: Push to GitHub
Lesson
Congratulations on reaching the end of the project. Now push your changes to the forked project repository on your GitHub and check the GitHub Action.

Don't forget to take a screenshot of the GitHub Action showing the passing workflows. Name it continuous_integration.png and upload it to the screenshots/ folder.

